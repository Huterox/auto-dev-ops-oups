"""
@FileNameï¼šRequirementsCollation.py
@Authorï¼šHuterox
@Descriptionï¼šGo For It
@Timeï¼š2024/7/20 15:41
@Copyrightï¼šÂ©2018-2024 awesome!
"""

import os
import toml
import time
import streamlit as st
import streamlit_antd_components as sac
from openai import OpenAI
import concurrent.futures
from base import current_dir_root
from bot.collection.agents import CollectionExtAgent
from webui.handler.collationHandler import getCollectionSummary, getCollectionSuggest, get_audio_content


@st.experimental_dialog('éŸ³é¢‘è§£æ',)
def audio_analysis(st,container_show):
    audio_analysis_container = st.container(height=300)
    with audio_analysis_container:
        audio_path = st.session_state.get("collation_audio_input")
        # å…ˆè¿‡ä¸€ä¸‹æç¤º
        if audio_path:
            if os.path.exists(audio_path):
                sac.alert(label='Tips',
                          description='éŸ³é¢‘å·²åŠ è½½',
                          size=12,
                          color='teal',
                          banner=False,
                          icon=True, closable=True)
            else:
                sac.alert(label='Tips',
                          description='éŸ³é¢‘åœ°å€é”™è¯¯',
                          size=12,
                          color='yellow',
                          banner=False,
                          icon=True, closable=True)
                return
            # ä¹‹ååœ¨å»è§£æéŸ³é¢‘
            # å±•ç¤ºè§£æç»“æœ
            sac.divider(label='`è§£æç»“æœ`', icon="lightning-charge", align='center', color='gray', key="5")
            audio_res_analysis_container = st.container(height=130)
            with audio_res_analysis_container:
                with st.spinner('æ­£åœ¨è§£æè¯­éŸ³...'):
                    with audio_res_analysis_container:
                        res,flag = get_audio_content(audio_path)
                        # å°†å½“å‰æå–çš„å¯¹è¯å†…å®¹å­˜å‚¨èµ·æ¥
                        st.session_state.current_audio_content = res
                        placeholder = st.empty()
                        full_response = ''
                        for item in res:
                            full_response += item
                            time.sleep(0.005)
                            placeholder.markdown(full_response)
                        placeholder.markdown(full_response)
                    if flag:
                        with st.spinner("æ­£åœ¨æ•´ç†éœ€æ±‚..."):
                            agent = CollectionExtAgent()
                            ext = agent.get_ext(res)
                            # ä¸´æ—¶å­˜å‚¨extï¼Œå¹¶å±•ç¤º
                            st.session_state["current_audio_content_ext"] = ext
                            container_show.empty()
                            with container_show.chat_message("assistant"):
                                placeholder = st.empty()
                                full_response = ''
                                for item in ext:
                                    full_response += item
                                    time.sleep(0.005)
                                    placeholder.markdown(full_response)
                                placeholder.markdown(full_response)
                            sac.alert(label='Tips',
                                      description='éœ€æ±‚æ•´ç†æå–å®Œæ¯•',
                                      size=12,
                                      color='teal',
                                      banner=False,
                                      icon=True, closable=True)


        else:
            sac.alert(label='Tips',
                      description='è¯·å¡«å†™éŸ³é¢‘åœ°å€',
                      size=12,
                      color='yellow',
                      banner=False,
                      icon=True, closable=True)


def collationUI():
    config = toml.load(os.path.join(current_dir_root, "api.toml"))
    t0,t1 = st.columns([1,3])
    with t0:
        st.subheader("MathAI V0.1")
        st.caption("POWERED BY @Huterox")
    with t1:
        sac.alert(label='Tips',
                  description=f'å¦‚æœå¯¹ç”Ÿæˆç»“æœæœ‰è¾ƒé«˜è¦æ±‚ï¼Œ'
                              f'`ä¾§è¾¹æ å¯æŠ˜å ï¼Œæ­¤å¤„é¡µé¢å°†å±•ç¤ºæ›´å¤šå†…å®¹`ã€‚'
                              f'å»ºè®®åœ¨ã€é¦–é¡µã€‘è®¾ç½®å¤„ï¼Œè®¾ç½®é«˜æ€§èƒ½æ¨¡å‹ğŸŒ'
                              f'\nç‚¹å‡»ç”Ÿæˆé€»è¾‘å›¾ï¼Œå¯ä»¥æ ¹æ®å½“å‰éœ€æ±‚ç”Ÿæˆé€»è¾‘å›¾'
                              f'\néŸ³é¢‘æå–å½“å‰åŸºäºWhisperå®ç°ï¼ˆå½“å‰å†…ç½®é›†æˆtinyæ¨¡å‹ï¼‰'
                  ,
                  color='success',
                  banner=False,
                  icon=True, closable=True)

    c0,c1,c2 = st.columns([1,2,1])
    with c0:
        # è¿™é‡Œæ˜¯ä¸¤ä¸ªæç¤ºæ€»ç»“æ¡†
        st.session_state["collation_summary_open"]=sac.switch(
            label='æ™ºèƒ½æ€»ç»“', align='center', size='md',
            value=st.session_state.get("collation_summary_open", False),
        )
        collation_summary_container = st.container(height=200)
        st.button("ç”Ÿæˆé€»è¾‘å›¾",type="primary")

        # å…ˆæ¸…ç©ºä¸€ä¸‹
        collation_summary_container.empty()
        if "collation_main_messages" not in st.session_state or st.session_state.collation_main_messages == []:
            with collation_summary_container.chat_message("assistant"):
                msg = "å½“å‰æ‚¨è¿˜æ²¡æœ‰æå‡ºéœ€æ±‚å–”~ï¼Œè¯·æ‚¨æå‡ºæ‚¨çš„éœ€æ±‚ï¼Œæˆ‘å°†ä¸ºæ‚¨æ€»ç»“éœ€æ±‚ğŸ˜„"
                placeholder = st.empty()
                full_response = ''
                for item in msg:
                    full_response += item
                    time.sleep(0.005)
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
        else:
            summary_content = st.session_state.get("collation_summary_content","æš‚æ— æ€»ç»“~")
            collation_summary_container.chat_message("assistant").write(summary_content)

        #######################################################################################
        st.session_state["collation_suggest_open"] = sac.switch(
            label='æ™ºèƒ½æç¤º', align='center', size='md',
            value=st.session_state.get("collation_suggest_open", False),
        )

        collation_suggest_container = st.container(height=200)
        # å…ˆæ¸…ç©ºä¸€ä¸‹
        collation_suggest_container.empty()
        if "collation_main_messages" not in st.session_state or st.session_state.collation_main_messages == []:
            with collation_suggest_container.chat_message("assistant"):
                msg = "å½“å‰æ‚¨è¿˜æ²¡æœ‰æå‡ºéœ€æ±‚å–”~ï¼Œè¯·æ‚¨æå‡ºæ‚¨çš„éœ€æ±‚ï¼Œæˆ‘å°†ä¸ºæ‚¨æå‡ºå»ºè®®ğŸ˜„"
                placeholder = st.empty()
                full_response = ''
                for item in msg:
                    full_response += item
                    time.sleep(0.005)
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
        else:
            suggest_content = st.session_state.get("collation_suggest_content","æš‚æ— å»ºè®®ğŸ‘€")
            collation_suggest_container.chat_message("assistant").write(suggest_content)

    with c1:
        # è¿™ä¸ªæ˜¯ä¸»å¯¹è¯æ¡†
        collation_main_container = st.container(height=500)
        if "collation_main_messages" not in st.session_state or st.session_state.collation_main_messages==[]:
            st.session_state["collation_main_messages"] = []
            collation_main_container.chat_message("assistant").write("æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå°åŠ©ç†ï¼Œæ‚¨å°†ä½œä¸ºç”¨æˆ·ä¸æˆ‘è¿›è¡Œéœ€æ±‚å¯¹æ¥ğŸ‘€")
        # åŠ è½½å†å²æ¶ˆæ¯
        for msg in st.session_state.collation_main_messages:
            collation_main_container.chat_message(msg["role"]).write(msg["content"])
        # åŠ è½½å¤§æ¨¡å‹ç›¸å…³é…ç½®
        default_key = config["DEFAULT"]["default_key"]
        default_base = config["DEFAULT"]["default_base"]
        default_model = config["DEFAULT"]["default_model"]
        default_temperature = config["DEFAULT"]["default_temperature"]
        if (default_base != None and default_model != ""):
            placeholder = "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„ä¹ˆï¼ŸğŸ˜€"
        else:
            placeholder = "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„ä¹ˆï¼ŸğŸ˜€(è¯·å…ˆè®¾ç½®é»˜è®¤å¤§æ¨¡å‹KEY)"
        if prompt := st.chat_input(placeholder=placeholder):
            client = OpenAI(api_key=default_key,
                            base_url=default_base,
                            )
            # æ·»åŠ å¯¹è¯å†å²ä¿¡æ¯
            st.session_state.collation_main_messages.append({"role": "user", "content": prompt})
            collation_main_container.chat_message("user").write(prompt)

            try:
                response = client.chat.completions.create(
                    model=default_model,
                    temperature=default_temperature,
                    messages=[
                        {"role": "system",
                         "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶å¼€å‘é¡¹ç›®ç»ç†ï¼Œæ¥ä¸‹æ¥ä½ å°†åŸºäºç”¨æˆ·çš„éœ€æ±‚æ¥ä¸ç”¨æˆ·è¿›è¡Œå¯¹æ¥\n"},
                        {"role": "user", "content": prompt}
                    ])
                msg = response.choices[0].message.content
                st.session_state.collation_main_messages.append({"role": "assistant", "content": msg})
                """
                åœ¨è¿™é‡Œå®Œæˆè¯·æ±‚ä¹‹åï¼Œæˆ‘ä»¬æ‹¿åˆ°historyï¼Œå»è¯¢é—®å¯¹åº”çš„agent
                """
                history = st.session_state.collation_main_messages
                # å¹¶å‘è¯·æ±‚ è·å–å»ºè®®å’Œæ€»ç»“
                # 2. åˆ›å»ºä»»åŠ¡,å¹¶æäº¤
                with concurrent.futures.ThreadPoolExecutor(
                        max_workers=2) as executor:  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°

                    # æäº¤éœ€æ±‚æ€»ç»“ä»»åŠ¡ï¼Œç”¨æˆ·éœ€æ±‚æé—®å»ºè®®ä»»åŠ¡
                    futures = [
                        executor.submit(getCollectionSummary, st,collation_summary_container,history),
                        executor.submit(getCollectionSuggest,st,collation_suggest_container,history)
                    ]
                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œç­‰å¾…æ‰€æœ‰ä¿¡æ¯å±•ç¤º
                    for future in concurrent.futures.as_completed(futures):
                        info = future.result()
            except Exception as e:
                msg = "æŠ±æ­‰ï¼Œå‡ºç°å¼‚å¸¸ï¼Œè¯·ç¨åå†è¯•~"
            with collation_main_container.chat_message("assistant"):
                placeholder = st.empty()
                full_response = ''
                for item in msg:
                    full_response += item
                    time.sleep(0.01)
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)

    with c2:
        # ä¾§è¾¹éŸ³é¢‘æå–æ€»ç»“ f'å½“å‰ä»…æ”¯æŒæ™®é€šè¯&è‹±æ–‡'
        sac.alert(label='Tips',
                  description=f'å½“å‰ä»…æ”¯æŒæ™®é€šè¯&è‹±æ–‡',
                  size=13,
                  color='success',
                  banner=False,
                  icon=True, closable=True)
        collation_audio_container = st.container(height=250)
        if not st.session_state.get("current_audio_content_ext"):
            collation_audio_container.empty()
            with collation_audio_container.chat_message("assistant"):
                msg = "è¯·æ‚¨è¾“å…¥éŸ³é¢‘æ–‡ä»¶åœ°å€ï¼Œæˆ‘å°†æ ¹æ®éŸ³é¢‘ä¸ºæ‚¨æ€»ç»“ç”¨æˆ·éœ€æ±‚ğŸ¥´"
                placeholder = st.empty()
                full_response = ''
                for item in msg:
                    full_response += item
                    time.sleep(0.01)
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
        else:
            collation_audio_container.chat_message("assistant")\
                .write(st.session_state.get("current_audio_content_ext"))
        audio_path = st.session_state.get("collation_audio_input")
        if audio_path:
            if os.path.exists(audio_path):
                st.audio(data=audio_path)
            else:
                st.markdown("`éŸ³é¢‘åœ°å€é”™è¯¯`")
        else:
            st.markdown("`æš‚æœªå¡«å†™éŸ³é¢‘æ–‡ä»¶åœ°å€`")
        audio_input_path = st.text_input("è¯·è¾“å…¥éŸ³é¢‘æ–‡ä»¶åœ°å€")
        st.session_state["collation_audio_input"] = audio_input_path
        if st.button("å¼€å§‹æå–éŸ³é¢‘å†…å®¹",type="primary"):
            audio_analysis(st,collation_audio_container)

