"""
@FileNameï¼šRequirementsCollation.py
@Authorï¼šHuterox
@Descriptionï¼šGo For It
@Timeï¼š2024/7/20 15:41
@Copyrightï¼šÂ©2018-2024 awesome!
"""

import os
import toml
import time
import streamlit as st
import streamlit_antd_components as sac
from openai import OpenAI
import concurrent.futures
from base import current_dir_root
from webui.handler.collationHandler import getCollectionSummary, getCollectionSuggest


def collationUI():
    config = toml.load(os.path.join(current_dir_root, "api.toml"))
    t0,t1 = st.columns([1,3])
    with t0:
        st.subheader("MathAI V0.1")
        st.caption("POWERED BY @Huterox")
    with t1:
        sac.alert(label='Tips',
                  description=f'å¦‚æœå¯¹ç”Ÿæˆç»“æœæœ‰è¾ƒé«˜è¦æ±‚ï¼Œ'
                              f'`ä¾§è¾¹æ å¯æŠ˜å ï¼Œæ­¤å¤„é¡µé¢å°†å±•ç¤ºæ›´å¤šå†…å®¹`ã€‚'
                              f'å»ºè®®åœ¨ã€é¦–é¡µã€‘è®¾ç½®å¤„ï¼Œè®¾ç½®é«˜æ€§èƒ½æ¨¡å‹ğŸŒ'
                              f'\nç‚¹å‡»ç”Ÿæˆé€»è¾‘å›¾ï¼Œå¯ä»¥æ ¹æ®å½“å‰éœ€æ±‚ç”Ÿæˆé€»è¾‘å›¾'
                              f'\néŸ³é¢‘æå–å½“å‰åŸºäºWhisperå®ç°ï¼ˆå½“å‰å†…ç½®é›†æˆtinyæ¨¡å‹ï¼‰'
                  ,
                  color='success',
                  banner=False,
                  icon=True, closable=True)

    c0,c1,c2 = st.columns([1,2,1])
    with c0:
        # è¿™é‡Œæ˜¯ä¸¤ä¸ªæç¤ºæ€»ç»“æ¡†
        st.session_state["collation_summary_open"]=sac.switch(
            label='æ™ºèƒ½æ€»ç»“', align='center', size='md',
            value=st.session_state.get("collation_summary_open", False),
        )
        collation_summary_container = st.container(height=200)
        st.button("ç”Ÿæˆé€»è¾‘å›¾",type="primary")

        # å…ˆæ¸…ç©ºä¸€ä¸‹
        collation_summary_container.empty()
        if "collation_main_messages" not in st.session_state or st.session_state.collation_main_messages == []:
            with collation_summary_container.chat_message("assistant"):
                msg = "å½“å‰æ‚¨è¿˜æ²¡æœ‰æå‡ºéœ€æ±‚å–”~ï¼Œè¯·æ‚¨æå‡ºæ‚¨çš„éœ€æ±‚ï¼Œæˆ‘å°†ä¸ºæ‚¨æ€»ç»“éœ€æ±‚ğŸ˜„"
                placeholder = st.empty()
                full_response = ''
                for item in msg:
                    full_response += item
                    time.sleep(0.005)
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
        else:
            summary_content = st.session_state.get("collation_summary_content","æš‚æ— æ€»ç»“~")
            if summary_content:
                with collation_summary_container.chat_message("assistant"):
                    placeholder = st.empty()
                    full_response = ''
                    for item in summary_content:
                        full_response += item
                        time.sleep(0.005)
                        placeholder.markdown(full_response)
                    placeholder.markdown(full_response)

        # ######################################################################################
        st.session_state["collation_suggest_open"] = sac.switch(
            label='æ™ºèƒ½æç¤º', align='center', size='md',
            value=st.session_state.get("collation_suggest_open", False),
        )

        collation_suggest_container = st.container(height=200)
        # å…ˆæ¸…ç©ºä¸€ä¸‹
        collation_suggest_container.empty()
        if "collation_main_messages" not in st.session_state or st.session_state.collation_main_messages == []:
            with collation_suggest_container.chat_message("assistant"):
                msg = "å½“å‰æ‚¨è¿˜æ²¡æœ‰æå‡ºéœ€æ±‚å–”~ï¼Œè¯·æ‚¨æå‡ºæ‚¨çš„éœ€æ±‚ï¼Œæˆ‘å°†ä¸ºæ‚¨æå‡ºå»ºè®®ğŸ˜„"
                placeholder = st.empty()
                full_response = ''
                for item in msg:
                    full_response += item
                    time.sleep(0.005)
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
        else:
            suggest_content = st.session_state.get("collation_suggest_content","æš‚æ— å»ºè®®ğŸ‘€")
            if suggest_content:
                with collation_suggest_container.chat_message("assistant"):
                    placeholder = st.empty()
                    full_response = ''
                    for item in suggest_content:
                        full_response += item
                        time.sleep(0.005)
                        placeholder.markdown(full_response)
                    placeholder.markdown(full_response)

    with c1:
        # è¿™ä¸ªæ˜¯ä¸»å¯¹è¯æ¡†
        collation_main_container = st.container(height=500)
        if "collation_main_messages" not in st.session_state or st.session_state.collation_main_messages==[]:
            st.session_state["collation_main_messages"] = []
            collation_main_container.chat_message("assistant").write("æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå°åŠ©ç†ï¼Œæ‚¨å°†ä½œä¸ºç”¨æˆ·ä¸æˆ‘è¿›è¡Œéœ€æ±‚å¯¹æ¥ğŸ‘€")
        # åŠ è½½å†å²æ¶ˆæ¯
        for msg in st.session_state.collation_main_messages:
            collation_main_container.chat_message(msg["role"]).write(msg["content"])
        # åŠ è½½å¤§æ¨¡å‹ç›¸å…³é…ç½®
        default_key = config["DEFAULT"]["default_key"]
        default_base = config["DEFAULT"]["default_base"]
        default_model = config["DEFAULT"]["default_model"]
        default_temperature = config["DEFAULT"]["default_temperature"]
        if (default_base != None and default_model != ""):
            placeholder = "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„ä¹ˆï¼ŸğŸ˜€"
        else:
            placeholder = "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„ä¹ˆï¼ŸğŸ˜€(è¯·å…ˆè®¾ç½®é»˜è®¤å¤§æ¨¡å‹KEY)"
        if prompt := st.chat_input(placeholder=placeholder):
            client = OpenAI(api_key=default_key,
                            base_url=default_base,
                            )
            # æ·»åŠ å¯¹è¯å†å²ä¿¡æ¯
            st.session_state.collation_main_messages.append({"role": "user", "content": prompt})
            collation_main_container.chat_message("user").write(prompt)
            response = client.chat.completions.create(
                model=default_model,
                temperature=default_temperature,
                messages=[
                    {"role": "system",
                     "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶å¼€å‘é¡¹ç›®ç»ç†ï¼Œæ¥ä¸‹æ¥ä½ å°†åŸºäºç”¨æˆ·çš„éœ€æ±‚æ¥ä¸ç”¨æˆ·è¿›è¡Œå¯¹æ¥\n"},
                    {"role": "user", "content": prompt}
                ])
            try:
                msg = response.choices[0].message.content
                st.session_state.collation_main_messages.append({"role": "assistant", "content": msg})
                """
                åœ¨è¿™é‡Œå®Œæˆè¯·æ±‚ä¹‹åï¼Œæˆ‘ä»¬æ‹¿åˆ°historyï¼Œå»è¯¢é—®å¯¹åº”çš„agent
                """
                history = st.session_state.collation_main_messages
                # å¹¶å‘è¯·æ±‚ è·å–å»ºè®®å’Œæ€»ç»“
                # 2. åˆ›å»ºä»»åŠ¡,å¹¶æäº¤
                with concurrent.futures.ThreadPoolExecutor(
                        max_workers=2) as executor:  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°

                    # æäº¤éœ€æ±‚æ€»ç»“ä»»åŠ¡ï¼Œç”¨æˆ·éœ€æ±‚æé—®å»ºè®®ä»»åŠ¡
                    futures = [
                        executor.submit(getCollectionSummary, st,collation_summary_container,history),
                        executor.submit(getCollectionSuggest,st,collation_suggest_container,history)
                    ]
                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œç­‰å¾…æ‰€æœ‰ä¿¡æ¯å±•ç¤º
                    for future in concurrent.futures.as_completed(futures):
                        info = future.result()
            except Exception as e:
                msg = "æŠ±æ­‰ï¼Œå‡ºç°å¼‚å¸¸ï¼Œè¯·ç¨åå†è¯•~"
            with collation_main_container.chat_message("assistant"):
                placeholder = st.empty()
                full_response = ''
                for item in msg:
                    full_response += item
                    time.sleep(0.01)
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)

    with c2:
        # ä¾§è¾¹éŸ³é¢‘æå–æ€»ç»“ f'å½“å‰ä»…æ”¯æŒæ™®é€šè¯&è‹±æ–‡'
        sac.alert(label='Tips',
                  description=f'å½“å‰ä»…æ”¯æŒæ™®é€šè¯&è‹±æ–‡',
                  size=13,
                  color='success',
                  banner=False,
                  icon=True, closable=True)
        collation_audio_container = st.container(height=250)
        with collation_audio_container.chat_message("assistant"):
            msg = "è¯·æ‚¨è¾“å…¥éŸ³é¢‘æ–‡ä»¶åœ°å€ï¼Œæˆ‘å°†æ ¹æ®éŸ³é¢‘ä¸ºæ‚¨æ€»ç»“ç”¨æˆ·éœ€æ±‚ğŸ¥´"
            placeholder = st.empty()
            full_response = ''
            for item in msg:
                full_response += item
                time.sleep(0.01)
                placeholder.markdown(full_response)
            placeholder.markdown(full_response)
        st.audio(data=None)
        st.text_input("è¯·è¾“å…¥éŸ³é¢‘æ–‡ä»¶åœ°å€", key="collation_audio_input")